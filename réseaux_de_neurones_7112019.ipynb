{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "##### Build a neural network with 4 layers (3 weight layers) using Python Programming where Mean Squared Error and sigmoid activation function should be employed as its loss function and activation function, respectively. Also, momentum term should be included in this basic Neural Network structure and Batch Gradient Descent should be used for training. The MNIST data set is available at http://yann.lecun.com/exdb/mnist/***\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "# data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd\n",
    "\n",
    "# scipy.special for sigmoid function\n",
    "import scipy.special\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Data import\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Layer class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # Input X is computed into the output Y for every layers\n",
    "    def forwardProp(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backwardProp(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backwardPropmom(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class Dense(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        self.weights = np.random.rand(inputSize, outputSize) - 0.5\n",
    "        self.bias = np.random.rand(1, outputSize) - 0.5\n",
    "        self.gamma=0.9\n",
    "        self.v=0\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forwardProp(self, inputData):\n",
    "        self.input = inputData\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given outputError=dE/dY. Returns inputError=dE/dX.\n",
    "    def backwardProp(self, outputError, learningRate):\n",
    "        inputError = np.dot(outputError, self.weights.T)\n",
    "        weightsError = np.dot(self.input.T, outputError)\n",
    "        # dBias = outputError\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learningRate * weightsError\n",
    "        self.bias -= learningRate * outputError\n",
    "        return inputError\n",
    "    \n",
    "    def backwardPropmom(self, outputError, learningRate):\n",
    "        inputError = np.dot(outputError, self.weights.T)\n",
    "        weightsError = np.dot(self.input.T, outputError)\n",
    "        # dBias = outputError\n",
    "\n",
    "        # update parameters\n",
    "        self.v = self.gamma*self.v -learningRate * weightsError\n",
    "        self.weights += self.v\n",
    "        self.bias -= learningRate * outputError\n",
    "        return inputError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivateLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forwardProp(self, inputData):\n",
    "        self.input = inputData\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns inputError=dE/dX for a given outputError=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backwardProp(self, outputError, learningRate):\n",
    "        return self.activation_prime(self.input) * outputError\n",
    "    def backwardPropmom(self, outputError, learningRate):\n",
    "        return self.activation_prime(self.input) * outputError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.mse_loss= None\n",
    "        self.mse_loss_derivative = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use_lossfunc(self, mse_loss, mse_loss_derivative):\n",
    "        self.mse_loss = mse_loss\n",
    "        self.mse_loss_derivative = mse_loss_derivative\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict_output(self, inputData):\n",
    "        # sample dimension first\n",
    "        samples = len(inputData)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = inputData[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forwardProp(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def accuracy(self,x_train,y_train,x_test,y_test):\n",
    "        train_accuracy=[]\n",
    "        test_accuracy=[]\n",
    "        size=len(x_train)\n",
    "        train_output=self.predict_output(x_train[0:size])\n",
    "        test_output=self.predict_output(x_test[0:size])\n",
    "        for i in range(1,size):\n",
    "            delta_train=mse_loss(y_train[0:i],train_output)\n",
    "            delta_test=mse_loss(y_train[0:i],test_output)\n",
    "            train_accuracy.append(delta_train)\n",
    "            test_accuracy.append(delta_test)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(train_accuracy, '-r', label=' Training accuracy')\n",
    "        ax.plot(test_accuracy, '-g', label=' Testing accuracy')\n",
    "        leg = ax.legend(title='Learning curves');\n",
    "\n",
    "        plt.show()\n",
    "        return train_accuracy,test_accuracy\n",
    "\n",
    "    # train the network\n",
    "    def train_network(self, x_train, y_train, epochs, learningRate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        errorList = []\n",
    "        for i in tqdm(range(epochs)):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forwardProp(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.mse_loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.mse_loss_derivative(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backwardProp(error, learningRate)\n",
    "                    \n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            errorList.append(err)\n",
    "            \n",
    "        # show error's graph\n",
    "        return errorList\n",
    "    \n",
    "    def train_network_mom(self, x_train, y_train, epochs, learningRate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        errorList = []\n",
    "        for i in tqdm(range(epochs)):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forwardProp(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.mse_loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.mse_loss_derivative(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backwardPropmom(error, learningRate)\n",
    "                    \n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            errorList.append(err)\n",
    "            \n",
    "        # show error's graph\n",
    "        return errorList\n",
    "    \n",
    "    def plot_learning_curves(self, X_train, X_test, y_train, y_test, size, iterations, learning_rate):\n",
    "        train_errors, test_errors = [], []\n",
    "\n",
    "        self.train_network(X_train[:size], y_train[:size], epochs=iterations, learningRate=learning_rate)\n",
    "\n",
    "        labels_train_predict = self.predict_output(X_train[:size])\n",
    "        labels_test_predict = self.predict_output(X_test[:size])\n",
    "\n",
    "        for m in range(1, size):\n",
    "            train_errors.append(mse_loss(y_train[:m], labels_train_predict))\n",
    "            test_errors.append(mse_loss(y_test[:m], labels_test_predict))\n",
    "\n",
    "        #plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "        #plt.plot(np.sqrt(test_errors), \"b-\", linewidth=3, label=\"test\")\n",
    "        #plt.legend(loc=\"upper right\", fontsize=14)\n",
    "        #plt.xlabel(\"Training set size\", fontsize=14)\n",
    "        #plt.plot(train_errors,label='train errors')\n",
    "        #plt.plot(test_errors,label='test errors')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork_mom:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.mse_loss= None\n",
    "        self.mse_loss_derivative = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use_lossfunc(self, mse_loss, mse_loss_derivative):\n",
    "        self.mse_loss = mse_loss\n",
    "        self.mse_loss_derivative = mse_loss_derivative\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict_output(self, inputData):\n",
    "        # sample dimension first\n",
    "        samples = len(inputData)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = inputData[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forwardProp(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def accuracy(self,x_train,y_train,x_test,y_test):\n",
    "        train_accuracy=[]\n",
    "        test_accuracy=[]\n",
    "        size=len(x_train)\n",
    "        train_output=self.predict_output(x_train[0:size])\n",
    "        test_output=self.predict_output(x_test[0:size])\n",
    "        for i in range(1,size):\n",
    "            delta_train=mse_loss(y_train[0:i],train_output)\n",
    "            delta_test=mse_loss(y_train[0:i],test_output)\n",
    "            train_accuracy.append(delta_train)\n",
    "            test_accuracy.append(delta_test)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(train_accuracy, '-r', label=' Training accuracy')\n",
    "        ax.plot(test_accuracy, '-g', label=' Testing accuracy')\n",
    "        leg = ax.legend(title='Learning curves');\n",
    "\n",
    "        plt.show()\n",
    "        return train_accuracy,test_accuracy\n",
    "\n",
    "    \n",
    "    def train_network_mom(self, x_train, y_train, epochs, learningRate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        errorList = []\n",
    "        for i in tqdm(range(epochs)):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forwardProp(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.mse_loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.mse_loss_derivative(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backwardPropmom(error, learningRate)\n",
    "                    \n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            errorList.append(err)\n",
    "            \n",
    "        # show error's graph\n",
    "        return errorList\n",
    "    \n",
    "    def plot_learning_curves(self, X_train, X_test, y_train, y_test, size, iterations, learning_rate):\n",
    "        train_errors, test_errors = [], []\n",
    "\n",
    "        self.train_network(X_train[:size], y_train[:size], epochs=iterations, learningRate=learning_rate)\n",
    "\n",
    "        labels_train_predict = self.predict_output(X_train[:size])\n",
    "        labels_test_predict = self.predict_output(X_test[:size])\n",
    "\n",
    "        for m in range(1, size):\n",
    "            train_errors.append(mse_loss(y_train[:m], labels_train_predict))\n",
    "            test_errors.append(mse_loss(y_test[:m], labels_test_predict))\n",
    "\n",
    "        #plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "        #plt.plot(np.sqrt(test_errors), \"b-\", linewidth=3, label=\"test\")\n",
    "        #plt.legend(loc=\"upper right\", fontsize=14)\n",
    "        #plt.xlabel(\"Training set size\", fontsize=14)\n",
    "        #plt.plot(train_errors,label='train errors')\n",
    "        #plt.plot(test_errors,label='test errors')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28*28)\n",
    "X_train = X_train.astype('float32')\n",
    "X_train /= 255\n",
    "# transform the output (number from 0 to 9) into a vector\n",
    "# for example: number 2 will become [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28*28)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Activation : Sigmoid Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoidFunction(x):\n",
    "    s = 1/(1+np.exp(-x))   \n",
    "    return s\n",
    "\n",
    "def sigmoidFunction_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1-s)\n",
    "    return ds\n",
    "\n",
    "# linespace generate an array from start and stop value, 100 elements\n",
    "values = np.linspace(-10,10,100)\n",
    "\n",
    "# prepare the plot, associate the color r(ed) or b(lue) and the label \n",
    "plt.plot(values, sigmoidFunction(values), 'r', label=\"Sigmoid Function\")\n",
    "plt.plot(values, sigmoidFunction_derivative(values), 'b',label=\"Sigmoid Function Derivative\")\n",
    "\n",
    "# Draw the grid line in background.\n",
    "plt.grid()\n",
    "\n",
    "# plt.plot(x)\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "\n",
    "# create the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Learning Curves***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def run_net(Layer1, Layer2, epochs):\n",
    "epochs=25\n",
    "#Network (3 weigthed layers)\n",
    "net = neuralNetwork()\n",
    "net.add_layer(Dense(28*28, 300))  # input_layer : 784 nodes\n",
    "net.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative)) # Hidden Layer 1 : 300 nodes\n",
    "net.add_layer(Dense(300, 100)) # Hidden Layer 2 : 100 nodes\n",
    "net.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net.add_layer(Dense(100, 10))\n",
    "net.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net.use_lossfunc(mse_loss, mse_loss_derivative)\n",
    "#network.accuracy(X_train[0:1000], y_train[0:1000],X_test[0:1000],y_test[0:1000])\n",
    "#predict_digit = run_net(300, 100, 25)\n",
    "\n",
    "net_mom=neuralNetwork_mom()\n",
    "net_mom.add_layer(Dense(28*28, 300))  # input_layer : 784 nodes\n",
    "net_mom.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative)) # Hidden Layer 1 : 300 nodes\n",
    "net_mom.add_layer(Dense(300, 100)) # Hidden Layer 2 : 100 nodes\n",
    "net_mom.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net_mom.add_layer(Dense(100, 10))\n",
    "net_mom.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net_mom.use_lossfunc(mse_loss, mse_loss_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Error on training ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/25 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▎                                                                               | 1/25 [00:08<03:21,  8.41s/it]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▋                                                                            | 2/25 [00:17<03:17,  8.57s/it]\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▉                                                                         | 3/25 [00:25<03:02,  8.32s/it]\n",
      "\n",
      "\n",
      "\n",
      " 16%|█████████████▎                                                                     | 4/25 [00:34<03:00,  8.59s/it]\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████████████▌                                                                  | 5/25 [00:42<02:52,  8.61s/it]\n",
      "\n",
      "\n",
      "\n",
      " 24%|███████████████████▉                                                               | 6/25 [00:51<02:41,  8.51s/it]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████████████████▏                                                           | 7/25 [00:58<02:23,  7.98s/it]\n",
      "\n",
      "\n",
      "\n",
      " 32%|██████████████████████████▌                                                        | 8/25 [01:07<02:21,  8.35s/it]\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████████████████████▉                                                     | 9/25 [01:14<02:08,  8.04s/it]\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████████████████████▊                                                 | 10/25 [01:22<02:01,  8.13s/it]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████████████████████████                                              | 11/25 [01:30<01:50,  7.89s/it]\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████████████████████████▎                                          | 12/25 [01:37<01:41,  7.78s/it]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████▋                                       | 13/25 [01:44<01:30,  7.58s/it]\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████████████████████████████▉                                    | 14/25 [01:53<01:27,  7.93s/it]\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████▏                                | 15/25 [02:00<01:17,  7.71s/it]\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████████████████████████████████▍                             | 16/25 [02:09<01:10,  7.87s/it]\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████▊                          | 17/25 [02:16<01:00,  7.62s/it]\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████                       | 18/25 [02:24<00:54,  7.76s/it]\n",
      "\n",
      "\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████████████▎                   | 19/25 [02:32<00:46,  7.82s/it]\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 20/25 [02:40<00:39,  7.93s/it]\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████████▉             | 21/25 [02:50<00:34,  8.54s/it]\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 22/25 [02:57<00:24,  8.02s/it]\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▍      | 23/25 [03:04<00:15,  7.71s/it]\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████▋   | 24/25 [03:11<00:07,  7.73s/it]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [03:19<00:00,  7.99s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/25 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|███▎                                                                               | 1/25 [00:10<04:10, 10.45s/it]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██████▋                                                                            | 2/25 [00:21<04:03, 10.59s/it]\n",
      "\n",
      "\n",
      "\n",
      " 12%|█████████▉                                                                         | 3/25 [00:32<03:58, 10.82s/it]\n",
      "\n",
      "\n",
      "\n",
      " 16%|█████████████▎                                                                     | 4/25 [00:46<04:05, 11.68s/it]\n",
      "\n",
      "\n",
      "\n",
      " 20%|████████████████▌                                                                  | 5/25 [00:57<03:50, 11.50s/it]\n",
      "\n",
      "\n",
      "\n",
      " 24%|███████████████████▉                                                               | 6/25 [01:09<03:39, 11.57s/it]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████████████████████▏                                                           | 7/25 [01:20<03:28, 11.58s/it]\n",
      "\n",
      "\n",
      "\n",
      " 32%|██████████████████████████▌                                                        | 8/25 [01:33<03:20, 11.79s/it]\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████████████████████████▉                                                     | 9/25 [01:44<03:07, 11.74s/it]\n",
      "\n",
      "\n",
      "\n",
      " 40%|████████████████████████████████▊                                                 | 10/25 [01:55<02:52, 11.53s/it]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████████████████████████████                                              | 11/25 [02:07<02:40, 11.44s/it]\n",
      "\n",
      "\n",
      "\n",
      " 48%|███████████████████████████████████████▎                                          | 12/25 [02:18<02:29, 11.46s/it]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████████████████████████████████▋                                       | 13/25 [02:30<02:19, 11.61s/it]\n",
      "\n",
      "\n",
      "\n",
      " 56%|█████████████████████████████████████████████▉                                    | 14/25 [02:42<02:08, 11.69s/it]\n",
      "\n",
      "\n",
      "\n",
      " 60%|█████████████████████████████████████████████████▏                                | 15/25 [02:53<01:55, 11.60s/it]\n",
      "\n",
      "\n",
      "\n",
      " 64%|████████████████████████████████████████████████████▍                             | 16/25 [03:05<01:45, 11.68s/it]\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████▊                          | 17/25 [03:17<01:33, 11.68s/it]\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████████████████████████████████████████████████████████                       | 18/25 [03:28<01:21, 11.64s/it]\n",
      "\n",
      "\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████████████▎                   | 19/25 [03:40<01:09, 11.56s/it]\n",
      "\n",
      "\n",
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 20/25 [03:52<00:58, 11.68s/it]\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████████▉             | 21/25 [04:04<00:47, 11.84s/it]\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 22/25 [04:15<00:34, 11.58s/it]\n",
      "\n",
      "\n",
      "\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▍      | 23/25 [04:25<00:22, 11.13s/it]"
     ]
    }
   ],
   "source": [
    "errorList = []\n",
    "errorList_mom=[]\n",
    "errorList.append(net.train_network(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "errorList_mom.append(net_mom.train_network_mom(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(errorList[0], '-r', label='Sigmoid')\n",
    "ax.plot(errorList_mom[0], '-g', label='Sigmoid w/ momentum')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Mean Squarred Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Error on testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errorList = []\n",
    "test_errorList_mom=[]\n",
    "test_errorList.append(net.train_network(X_test[0:1000], y_test[0:1000], epochs, learningRate=0.1))\n",
    "test_errorList_mom.append(net_mom.train_network_mom(X_test[0:1000], y_test[0:1000], epochs, learningRate=0.1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_errorList[0], '-r', label='Sigmoid')\n",
    "ax.plot(test_errorList_mom[0], '-g', label='Sigmoid w/ momentum')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Mean Squarred Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Activation Function : ReLU\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def dReLU(x):\n",
    "    x[x<0]=0\n",
    "    x[x>0]=1\n",
    "    return x\n",
    "\n",
    "# linespace generate an array from start and stop value, 100 elements\n",
    "values = np.linspace(-10,10,100)\n",
    "\n",
    "# prepare the plot, associate the color r(ed) or b(lue) and the label \n",
    "plt.plot(values, ReLU(values), 'r', label=\" ReLU Function\")\n",
    "plt.plot(values, dReLU(values), 'b',label=\"ReLU Function Derivative\")\n",
    "# Draw the grid line in background.\n",
    "plt.grid()\n",
    "# plt.plot(x)\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "# create the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network (3 weigthed layers)\n",
    "net_relu= neuralNetwork()\n",
    "net_relu.add_layer(Dense(28*28, 300))\n",
    "net_relu.add_layer(ActivateLayer(ReLU, dReLU))\n",
    "net_relu.add_layer(Dense(300, 100))\n",
    "net_relu.add_layer(ActivateLayer(ReLU, dReLU))\n",
    "net_relu.add_layer(Dense(100, 10))\n",
    "net_relu.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net_relu.use_lossfunc(mse_loss, mse_loss_derivative)\n",
    "#network.accuracy(X_train[0:1000], y_train[0:1000],X_test[0:1000],y_test[0:1000])\n",
    "net_relu_mom= neuralNetwork_mom()\n",
    "net_relu_mom.add_layer(Dense(28*28, 300))\n",
    "net_relu_mom.add_layer(ActivateLayer(ReLU, dReLU))\n",
    "net_relu_mom.add_layer(Dense(300, 100))\n",
    "net_relu_mom.add_layer(ActivateLayer(ReLU, dReLU))\n",
    "net_relu_mom.add_layer(Dense(100, 10))\n",
    "net_relu_mom.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "net_relu_mom.use_lossfunc(mse_loss, mse_loss_derivative)\n",
    "\n",
    "\n",
    "    \n",
    "# input_layer : 784 nodes\n",
    "# Hidden Layer 1 : 300 nodes\n",
    "# Hidden Layer 2 : 100 nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Error on training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errorList_relu=[]\n",
    "train_errorList_relu_mom=[]\n",
    "train_errorList_relu.append(net_relu.train_network(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "train_errorList_relu_mom.append(net_relu_mom.train_network_mom(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_errorList_relu[0], '-r', label='ReLU')\n",
    "ax.plot(train_errorList_relu_mom[0], '-g', label='ReLU w/ momentum')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Mean Squarred Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error on testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errorList_relu=[]\n",
    "test_errorList_relu_mom=[]\n",
    "test_errorList_relu.append(net_relu.train_network(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "test_errorList_relu_mom.append(net_relu_mom.train_network_mom(X_train[0:1000], y_train[0:1000], epochs, learningRate=0.1))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_errorList_relu[0], '-r', label='ReLU')\n",
    "ax.plot(test_errorList_relu_mom[0], '-g', label='ReLU w/ momentum')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Mean Squarred Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "C. Stochastic Gradient Descent vs Mini Batch Gradient Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net_sgd(Layer1, Layer2, epochs):\n",
    "    errorList=[]\n",
    "    errorList_mom=[]\n",
    "    # Network (3 weigthed layers)\n",
    "    network = neuralNetwork()\n",
    "    network.add_layer(Dense(28*28, Layer1))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "    network.add_layer(Dense(Layer1, Layer2))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "    network.add_layer(Dense(Layer2, 10))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "\n",
    "    # train on 1000 samples\n",
    "    # training will be slow if we update at each iteration on 60000 samples\n",
    "    # compromise with 250 epochs for the precision & time with L1 of 100 and L2 of 50 neurons\n",
    "    network.use_lossfunc(mse_loss, mse_loss_derivative)\n",
    "    for epoch in range(3):\n",
    "        for i in range(1000):\n",
    "            random_idx=np.random.randint(0,60000)\n",
    "            errorList.append(network.train_network(X_train[random_idx:random_idx+1], y_train[random_idx:random_idx+1],1, learningRate=0.1))\n",
    "            errorList_mom.append(network.train_network_mom(X_train[random_idx:random_idx+1], y_train[random_idx:random_idx+1],1, learningRate=0.1))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(errorList[0], '-r', label='SGD ')\n",
    "    ax.plot(errorList_mom[0], '-g', label='SGD w/momentum')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Mean Squarred Error')\n",
    "    plt.show()\n",
    "predict_digit = run_net_sgd(300, 100, 25)\n",
    "# input_layer : 784 nodes\n",
    "# Hidden Layer 1 : 300 nodes\n",
    "# Hidden Layer 2 : 100 nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Mini Batch Gradient Descent\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_net_mbgd(Layer1, Layer2, epochs):\n",
    "    errorList=[]\n",
    "    # Network (3 weigthed layers)\n",
    "    network = neuralNetwork()\n",
    "    network.add_layer(Dense(28*28, Layer1))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "    network.add_layer(Dense(Layer1, Layer2))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "    network.add_layer(Dense(Layer2, 10))\n",
    "    network.add_layer(ActivateLayer(sigmoidFunction, sigmoidFunction_derivative))\n",
    "\n",
    "    # train on 1000 samples\n",
    "    # training will be slow if we update at each iteration on 60000 samples\n",
    "    # compromise with 250 epochs for the precision & time with L1 of 100 and L2 of 50 neurons\n",
    "    network.use_lossfunc(mse_loss, mse_loss_derivative)\n",
    "    X_train_mbgd = X_train\n",
    "    y_train_mbgd = y_train\n",
    "    epochs=3\n",
    "    mini_batch_size=300\n",
    "    no_batches=2\n",
    "    samples=int(mini_batch_size/no_batches)\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(1000):\n",
    "            shuffle = np.random.permutation(60000)\n",
    "            shuffle=shuffle[:samples]\n",
    "            \n",
    "            X_train_mbgd = X_train[shuffle]\n",
    "            y_train_mbgd=y_train[shuffle]\n",
    "            errorList.append(network.train_network_mom(X_train_mbgd, y_train_mbgd,1, learningRate=0.1))\n",
    "    plt.plot(errorList[0])\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "predict_digit = run_net_sgd(300, 100, 25)\n",
    "# input_layer : 784 nodes\n",
    "# Hidden Layer 1 : 300 nodes\n",
    "# Hidden Layer 2 : 100 nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
